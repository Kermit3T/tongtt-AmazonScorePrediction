{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set all random seeds\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Files\n",
    "\n",
    "Download the csv files into the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = pd.read_csv(\"./data/train.csv\")\n",
    "testingSet = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(\"train.csv shape is \", trainingSet.shape)\n",
    "print(\"test.csv shape is \", testingSet.shape)\n",
    "\n",
    "print(\"\\nFirst few rows of training set:\")\n",
    "print(trainingSet.head())\n",
    "print(\"\\nFirst few rows of test set:\")\n",
    "print(testingSet.head())\n",
    "\n",
    "print(\"\\nTraining set statistics:\")\n",
    "print(trainingSet.describe())\n",
    "\n",
    "# Plot score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "trainingSet['Score'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Review Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase and remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    return ''\n",
    "\n",
    "def add_features_to(df):\n",
    "    # Text length features\n",
    "    df['text_length'] = df['Text'].fillna('').apply(len)\n",
    "    df['summary_length'] = df['Summary'].fillna('').apply(len)\n",
    "    \n",
    "    # Helpfulness ratio with handling division by zero\n",
    "    df['helpfulness_ratio'] = df['HelpfulnessNumerator'] / df['HelpfulnessDenominator'].replace(0, 1)\n",
    "    df['helpfulness_ratio'] = df['helpfulness_ratio'].fillna(0)\n",
    "    \n",
    "    # User statistics\n",
    "    user_stats = df.groupby('UserId').agg({\n",
    "        'Score': ['mean', 'count']\n",
    "    }).fillna(0)\n",
    "    user_stats.columns = ['user_avg_score', 'user_review_count']\n",
    "    df = df.merge(user_stats, left_on='UserId', right_index=True, how='left')\n",
    "    \n",
    "    # Product statistics\n",
    "    product_stats = df.groupby('ProductId').agg({\n",
    "        'Score': ['mean', 'count']\n",
    "    }).fillna(0)\n",
    "    product_stats.columns = ['product_avg_score', 'product_review_count']\n",
    "    df = df.merge(product_stats, left_on='ProductId', right_index=True, how='left')\n",
    "    \n",
    "    # Text preprocessing\n",
    "    df['processed_text'] = df['Text'].fillna('').apply(preprocess_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the feature extracted files if they've already been generated\n",
    "if exists('./data/X_train.csv'):\n",
    "    X_train = pd.read_csv(\"./data/X_train.csv\")\n",
    "    X_submission = pd.read_csv(\"./data/X_submission.csv\")\n",
    "else:\n",
    "    # Process the DataFrame\n",
    "    train = add_features_to(trainingSet)\n",
    "    \n",
    "    # Merge on Id so that the submission set can have feature columns as well\n",
    "    X_submission = pd.merge(train, testingSet, left_on='Id', right_on='Id')\n",
    "    X_submission = X_submission.drop(columns=['Score_x'])\n",
    "    X_submission = X_submission.rename(columns={'Score_y': 'Score'})\n",
    "    \n",
    "    # The training set is where the score is not null\n",
    "    X_train = train[train['Score'].notnull()]\n",
    "    \n",
    "    # Save processed datasets\n",
    "    X_submission.to_csv(\"./data/X_submission.csv\", index=False)\n",
    "    X_train.to_csv(\"./data/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample + Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set into training and testing set\n",
    "X_train_split, X_test_split, Y_train, Y_test = train_test_split(\n",
    "    X_train.drop(columns=['Score']),\n",
    "    X_train['Score'],\n",
    "    test_size=1/4.0,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'text_length', 'summary_length', 'helpfulness_ratio',\n",
    "    'user_avg_score', 'user_review_count',\n",
    "    'product_avg_score', 'product_review_count'\n",
    "]#calculate these\n",
    "\n",
    "# Create TF-IDF features with fixed random state\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000, \n",
    "    ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "# Get feature importance using a simple model\n",
    "X_train_select = X_train_split[feature_columns]\n",
    "X_test_select = X_test_split[feature_columns]\n",
    "X_submission_select = X_submission[feature_columns]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_select)\n",
    "X_test_scaled = scaler.transform(X_test_select)\n",
    "X_submission_scaled = scaler.transform(X_submission_select)\n",
    "\n",
    "# Train a model to get feature importance\n",
    "rf_feature = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_feature.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_feature.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the final model with fixed random state\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Y_test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy on testing set = \", accuracy_score(Y_test, Y_test_predictions))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Id': X_submission['Id'],\n",
    "    'Score': model.predict(X_submission_scaled)\n",
    "})\n",
    "\n",
    "submission.to_csv(\"./data/submission.csv\", index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
